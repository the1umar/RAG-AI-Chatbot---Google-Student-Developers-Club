{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7Buub55LHGB5"
      },
      "outputs": [],
      "source": [
        "!pip install langchain chromadb pypdf langchain_community sentence_transformers langchain_huggingface pyngrok streamlit langchain-groq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmn75TSQiM3E",
        "outputId": "0a95f021-7340-4349-978d-f21b5f138ef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        " #This exports all content in this to an app.py file which ngrok will use to run the app\n",
        "%%writefile app.py\n",
        "import os\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "import streamlit as st\n",
        "\n",
        "# Define constants\n",
        "GROQ_API_KEY = \"\"  # Set your Groq API key here\n",
        "PERSIST_DIRECTORY = \"/content/chroma_db/\"\n",
        "folder_path=\"/content/data\"\n",
        "DOCUMENT_PATH = \"/content/data/2307.pdf\"  # Make sure this path is correct\n",
        "# Here we define the title for our page or chatbot interface\n",
        "st.title(\"RAG Chatbot Interface\")\n",
        "\n",
        "# Remove caching from this function\n",
        "def load_or_create_vector_store(texts, embeddings):\n",
        "    \"\"\"Load or create a new Chroma vector store.\"\"\"\n",
        "    if os.path.exists(PERSIST_DIRECTORY):\n",
        "        return Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)\n",
        "\n",
        "    return Chroma.from_documents(texts, embeddings, persist_directory=PERSIST_DIRECTORY)\n",
        "\n",
        "def load_documents(document_path):\n",
        "    \"\"\"Load and split documents.\"\"\"\n",
        "    #  use text loader for loading csv or txt files\n",
        "    all_texts = []  # To store the texts from all PDFs\n",
        "    # Iterate over each file in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        # Only process files that end with .pdf\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            loader = PyPDFLoader(file_path)  # Load the individual PDF\n",
        "            documents = loader.load()  # Load documents from the PDF\n",
        "\n",
        "            # Split the loaded documents into smaller chunks\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200,\n",
        "                length_function=len,\n",
        "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "            )\n",
        "            texts = text_splitter.split_documents(documents)\n",
        "            all_texts.extend(texts)\n",
        "    return all_texts\n",
        "# Load documents and create embeddings\n",
        "loading_message = st.empty()  # Create an empty placeholder for the loading message\n",
        "loading_message.text(\"Loading documents and setting up embeddings...\")  # Set the initial loading message\n",
        "\n",
        "texts = load_documents(DOCUMENT_PATH)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/msmarco-distilbert-base-v4\")\n",
        "db = load_or_create_vector_store(texts, embeddings)\n",
        "db.persist()\n",
        "\n",
        "# Clear the loading message\n",
        "loading_message.empty()  # Remove the loading message\n",
        "\n",
        "# Set up the language model with the API key\n",
        "llm = ChatGroq(model=\"llama-3.1-70b-versatile\", api_key=GROQ_API_KEY)  # Pass the API key here\n",
        "\n",
        "# Define the prompt template\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer: \"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
        "\n",
        "# Set up the RAG pipeline\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=db.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Accept user input\n",
        "if prompt := st.chat_input(\"What is up?\"):\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Get the assistant's response\n",
        "    response = qa_chain({\"query\": prompt})  # Use the prompt to get the response\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response['result']})\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response['result'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ynYfPlKih1C",
        "outputId": "b04aa887-8145-4449-927c-da7f79cbd4a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            " * Tunnel URL: https://17b3-35-247-9-37.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set authentication token if you haven't already done so\n",
        "ngrok.set_auth_token(\"\")\n",
        "\n",
        "# Start Streamlit server on a specific port\n",
        "!nohup streamlit run app.py --server.port 5011 &\n",
        "\n",
        "# Start ngrok tunnel to expose the Streamlit server\n",
        "ngrok_tunnel = ngrok.connect(addr='5011', proto='http', bind_tls=True)\n",
        "\n",
        "# Print the URL of the ngrok tunnel\n",
        "print(' * Tunnel URL:', ngrok_tunnel.public_url)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
